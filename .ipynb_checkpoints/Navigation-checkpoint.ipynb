{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we will train an agent to play a game of banana catching. The goal is to only catch yellow bananas and avoid the blue bananas.\n",
    "\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from rl_environment import RLEnvironment\n",
    "from base_agent import BaseAgent\n",
    "from dqn_agent import DQNAgent\n",
    "from double_dqn_agent import DoubleDQNAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set up the hyperparameters for the environment and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = {\"base\": BaseAgent, \"random\": BaseAgent, \"dqn\": DQNAgent, \"double_dqn\": DoubleDQNAgent}\n",
    "\n",
    "configuration = {\n",
    "                \"max_episodes\" : 20,\n",
    "                \"max_time\" : 300, \n",
    "                \"eps_start\" : 1.0,\n",
    "                \"eps_end\" : 0.01,\n",
    "                \"eps_decay\" : 0.990,\n",
    "                \"target_score\" : 100.0,\n",
    "                \"agent\" : {\n",
    "                        \"type\" : \"double_dqn\",\n",
    "                        \"buffer_size\" : int(1e5),  # replay buffer size\n",
    "                        \"batch_size\" : 128,         # minibatch size\n",
    "                        \"gamma\" : 0.99,            # discount factor\n",
    "                        \"tau\" : 1e-3,              # for soft update of target parameters\n",
    "                        \"lr\" : 5e-4,               # learning rate \n",
    "                        \"update_every\" : 4,        # how often to update the network\n",
    "                        \"network\" :{\"hidden_layers\": [64,64]} #amount and size of hidden layers\n",
    "                }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, download the required files\n",
    "\n",
    "\n",
    "- [Linux](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux.zip)\n",
    "- [Mac OSX](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana.app.zip)\n",
    "- [Windows (32-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86.zip)\n",
    "- [Windows (64-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86_64.zip)\n",
    "\n",
    "\n",
    "Afterwards change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RLEnvironment(file_name=\"Banana_Windows_x86_64/Banana.exe\")\n",
    "env.print_env_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Try out other hyperparameters and compare them\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_result = {\"name\": name, \"type\":configuration[\"agent\"][\"type\"],\n",
    "                \"date\": start_date.strftime(\"%Y-%m-%d %H:%M:%S\"), \"episodes\" : len(scores),\n",
    "                \"final_score\" :  sum(scores[-100:])/len(scores[-100:]), \"duration\" : str(duration),\n",
    "                \"scores\": scores, \"configuration\": configuration, \"model_path\" : model_path}\n",
    "\n",
    "# save the result of the current run\n",
    "with open(\"output/results_\"+ name + \".json\", 'w') as f:\n",
    "    json.dump(cur_result, f, indent=2) \n",
    "\n",
    "# open all previous results\n",
    "if os.path.isfile(\"output/results.json\"):\n",
    "    with open(\"output/results.json\", 'r') as f:\n",
    "        results = json.load(f)\n",
    "# if it does not exist create an empty array\n",
    "else:\n",
    "    results = []\n",
    "\n",
    "# save results with the current result appended\n",
    "results.append(cur_result)\n",
    "with open(\"output/results.json\", 'w') as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will run all the configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# plot the results and compare them with other results\n",
    "def calculate_moving_average(numbers, window_size):\n",
    "    return [round(sum(numbers[max(0,i-window_size):i]) / \n",
    "                  min(i,window_size),\n",
    "                  2)\n",
    "            for i in range(1, len(numbers)+1)]\n",
    "\n",
    "plt.plot(calculate_moving_average(scores,100), color='red', label='current result')\n",
    "\n",
    "#filter the results on the 5 best final score (highest average of last 100 episodes) \n",
    "results_displayed = sorted(results, key=lambda r: r['date'], reverse=True)[:5]\n",
    "cmap = plt.cm.get_cmap('hsv', len(results_displayed)+2)\n",
    "#plot these results by taking all the scores and calculating the moving average.\n",
    "for i, result in enumerate(results_displayed):\n",
    "    label = result['type'] + '_' + result['date'] if result.get('alias') is None else result['alias']\n",
    "    plt.plot(calculate_moving_average(result['scores'],100), color=cmap(i+1), alpha=1.0, linestyle='--', label=label)\n",
    "plt.legend()\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Episode #\")\n",
    "# plt.show()\n",
    "\n",
    "# close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Watch the trained agent\n",
    "\n",
    "In the next code cell, you will load the trained weights from file to watch a smart agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "\n",
    "# agent.load()\n",
    "\n",
    "score = 0\n",
    "for j in range(2000):\n",
    "    action = agent.act(state)\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#close the unity environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
